%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn,paper=letter,fontsize=11pt]{article}
\usepackage[cm]{fullpage} 

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\setlength\parindent{0pt}
\usepackage[protrusion=true,expansion=true]{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

%\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{multirow}

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
%\fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{graphicx} % Including graphics
\usepackage{subcaption} % Including subfigures
\usepackage{stfloats} 
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Large\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Applying Machine Learning to Predict and Explain Primate Consortship} % Article title
\author{%
\textsc{Josh King} \\[1ex] 
\and 
\textsc{Vayu Kishore} \\[1ex] 
\and 
\textsc{Filippo Ranalli} \\[1ex] 
}
\date{} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
%\begin{abstract}
%\noindent \blindtext % Dummy abstract text - replace \blindtext with your abstract text
%\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle
%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

% \lettrine[nindent=0em,lines=3]{T} he mating behavior of a species drives genetic
% interchange and is a major driver of evolution within populations. Factors that
% contribute to consortship may range from genetic causes, such as physical traits
% expressed due to the presence of a gene, to those that are social or behavioral.
% The contribution of social factors is of particular interest for animals that
% live in structured groups, where factors such as rank within a social heirarchy
% may impact mating success.  Investigating this topic will provide deeper insight
% into the factors of mating success in social mammals, which may be extended to
% apply to humans as well.\\

\lettrine[nindent=0em,lines=3]{W}e apply machine learning methods to
investigate the behavioral and genetic reasons for success and failure of
mating between wild baboon pairs. Our analysis applies machine learning methods
to examine whether successful consortships can be predicted, and whether
certain behavioral or genetic features are especially relevant in determining
consortship. Additionally, we investigate whether successful and unsuccessful
pairs can be clustered, and whether there are certain clusters present in one
group that are not present in the other.

%------------------------------------------------

\section{Related Work}

\cite{Tung:2012} examine the impact of behavioral and genetic effects on
consortship in male/female baboon pairs in a wild yellow baboon population. They
found that a mix of both genetic and social factors drive non-random mating
within populations.

\section{Dataset}
The dataset that we use was collected and analyzed using statistical methods by
\cite{Tung:2012}. It contains observations about the success of potential baboon
mating pairs. In particular, it specifies if , male/female baboon pairings were
successful in consorting when given the opportunity.  There are approximately
12,000 observations, and the features are a mix of behavioral data such as the
rank difference between the male and female pair, as well as genetic data, such
as the estimated genetic distance between the pair. 
% \subsection{Features}
There are several categories of features present within the dataset.
\begin{itemize}
  \item{\textbf{Observed biological and genetic features}: These include female age
    and conceptiveness, as well as the estimated genetic diversity and genetic
    distance between the mating pair.}
  \item{\textbf{Observed behavioral features}: These include rank of the mating pair
    within the social heirarchy of the group and how many males and females from
    their group were present when the interaction of the pair took place.}
  \item{\textbf{Derived Pairwise Features}: Some features are
    transformations applied to the observed features described above, which were
    computed by \cite{Tung:2012}. Some of the transformations compute a pairwise
    score based on a combination of the male and female attributes. For example,
    the rank\_interact feature represents the combination of male and female
    ranks.
    }
  \item{\textbf{Derived Single Features}: The remaining features are single
    raw features which are scaled based on an assumed distribution.  The
    male\_rank\_transform feature, for example, scales the male rank based on an
    exponential distribution.}
\end{itemize}

In addition to the different categories, some of the features, such as
(untransformed) rank, are ordinal, while others, such as estimated genetic
distance are real-valued.\\

% \subsection{Identifiers and Labels}
Each datapoint contains an identifier of the male and female pair in an
interaction that could lead to a consortship and a 0/1 label which indicates
whether the male and female consorted.

% \section{Preprocessing}

% In the analysis that we have conducted up to this point, we have not performed
% any preprocessing on the data. We may consider applying additional
% transformations to the features, such as whitening using PCA if we find that our
% models are negatively impacted by the colinearity between features.

\section{Methods}
% \subsection{Materials}

% To perform our analysis, we used the \texttt{scikit-learn} python package. Methods
% that were not readily available in \texttt{scikit-learn}, such as feature selection, were
% implemented independently in python.

\subsection{Classification}
To examine if we can predict consortship, we perform a K-fold validation over
several linear and non-linear classifiers, each embedded in a forward and
backward feature selection algorithm framework.\\

The linear classifiers examined are logistic regression and linear SVM, while
the non-linear classifiers are gaussian and polynomial SVM. For each algorithm,
we tuned the hyper-parameters (such as the regularization parameter) and we
ensured that the class weights were balanced, given that the negative
(non-consort) labels outweigh the positive labels (consort) by 5-to-1. When
performing K-fold validation with 4 folds, the data was randomly shuffled so as
to avoid positive labels concentrating in a particular training or test set,
biasing the performance. Moreover, for each algorithm and feature subset we
evaluate the error, precision, recall and F1 score as an average on each of
the K-cluster training and test sets. Finally we store the confusion matrix,
obtained by running 75-25 hold-out cross-validation with all the features and
for the models that performed best.

\subsection{Clustering}
% kmeans on non-consort pairs, examined centroids, used PCA to visualize

We ran the k-means algorithm with 4 clusters on the consorting and
non-consorting pairs. To visualize the clusters, we then used dimensionality
reduction techniques such as PCA and t-SNE manifold tool. We also examined
using affinity propagation for clustering which can automatically determine the
number of clusters.

\subsection{Graphical Approach}

%------------------------------------------------
Another approach to predicting consortship is to use the interactions as edges
and the baboons as nodes. Due to only heterosexual interactions, the resulting
graph is bipartite. This transforms the classification task to an edge
prediction task. Most previous work on this has been on just predicting whether
or not edges exist, not predicting the label of the edge. We adapted current
methods to predict the label of an edge rather than its existence.\\ 

\cite{Macskassy:2007} use a simple method that computes the similarities of the
attributes of nodes to determine the weight of latent edges between nodes.
Because most of the features are not inherent to the baboons, but are
depdendent on the interaction between individuals, we adapt this method to look
at the similarity between the proposed edge and already existing edges between
those two nodes. More formally, if there is some set of nodes $V$ and some set
of edges $E$ where each edge $e_i$ has some associated features $f_i$ and some
associated label $l_i\in \{0,1\}$ then to determine the label of a new edge
$e_n$ between nodes $u$ and $v$ we would compute $$argmax_{label \in \{0,1\}}
\frac{\sum_{e_i \in E}1\{l_i=label\}\cdot|e-e_n|_2}{\sum_{e_i \in
E}1\{l_i=label\}}$$ We implemented this method, and also at a more specific
method where, instead of looking at all edges, we only look at edges where $u$
or $v$ are one of the endpoints of the edge. Both of these models are similar
to locally weighted regression which we implemented as well. \\

\section{Results and Discussion}
\subsection{Exploratory Data Analysis}
%\begin{figure}
      %\centering
          %\includegraphics[width=0.5\textwidth]{../figs/all_feats_histogram.png}
  %\caption{Distribution of features}
  %\label{fig:figure_distrib}
%\end{figure}
The features within the dataset seem to have varying distributions. While some
features, such as the number of males present, seem to have a balanced
distribution, others, such as the ages and ranks, show a heavy skew with long
tails.\\

%\begin{figure}[h]
      %\centering
          %\includegraphics[width=0.5\textwidth]{../figs/consort_distrib.png}
  %\caption{Distribution of consort/non-consort labels}
  %\label{fig:label_distrib}
%\end{figure}
Also, we observe that the labels within the dataset are imbalanced -- there are
approximately 5 times as many pairs which did not consort than those who did.
This imbalance may cause an issue in our classification algorithms. To avoid
this we use the \texttt{weight} for \texttt{scikit-learn}'s classifiers to
automatically assign weights to the classes based on the incidence of labels.\\
% PCA results
\begin{figure}
      \centering
          \includegraphics[width=0.5\textwidth]{../figs/consort_non_consort_visualization_pca.png}
  \caption{Reduced-dimensionality PCA visualization of consorting (red) and
  non-consorting pairs (green)}
  \label{fig:pca_vis}
\end{figure}

Using PCA to map higher dimensional dataset to 2 dimensions does not reveal
any lower-dimensional separation between the consorting/non-consorting pairs, as
shown in Figure \ref{fig:pca_vis}.
This indicates that the classification problem may be inherently
high-dimensional. The tSNE manifold mapping indicates this as well.
\subsection{Classification}
% confusion matrix

Running forward and backward feature selection shows that, for all the
classifiers considered, the best performance in terms of the designated metrics
is achieved when all the 16 derived and non-derived features are present.
Moreover when breaking down the performance of each classifier, the top pick is
gaussian SVM, followed by linear SVM. The metrics for the former algorithm are
presented in the following table, along with the confusion matrices for the
best and second-best classifiers. Interestingly, while the gaussian SVM
performs better overall in the metrics, linear SVM has a far higher rate of
correct prediction on the positive labels. While the average errors with
gaussian SVM are relatively high, the Fischer scores clearly showcase a poor
weighed average of precision and recall, indicating this algorithm does not
perform well.\\
\begin{table}[h]
  \centering
  \begin{tabular}{| l |l |c |c|}
    \hline
    & & \multicolumn{2}{ |c| }{Predicted} \\
    \hline
    & & Non-Consort & Consort \\
    \hline
    \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{True}}} & Non-Consort & 0.827 & 0.173  \\
                                                                     & Consort & 0.632 & 0.376 \\
    \hline
  \end{tabular}
  \caption{Gaussian SVM normalized confusion matrix in 75-25 cross-validation.}
  \label{tbl:cm_rbf_vis}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{| l |l |c |c|}
    \hline
    & & \multicolumn{2}{ |c| }{Predicted} \\
    \hline
    & & Non-Consort & Consort \\
    \hline
    \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{True}}} & Non-Consort & 0.537 & 0.463  \\
                                                                     & Consort & 0.277 & 0.723 \\
    \hline
  \end{tabular}
  \caption{Linear SVM confusion matrix in 75-25 cross-validation.}
  \label{tbl:cm_lin_vis}
\end{table}
\begin{figure}
      \centering
          \includegraphics[width=0.5\textwidth]{../figs/ClassificationMetrics_RbfSVM.png}
  \caption{Gaussian SVM metrics based on number of features in forward feature selection.}
  \label{fig:rbf_svm_vis}
\end{figure}


\subsection{Clustering}
% kmeans graphic
% \begin{figure}[h]
      % \centering
          % \includegraphics[width=0.5\textwidth]{../figs/non_consort_kmeans_2d_tsne.png}
  % \caption{t-SNE visualization of K-Means with 4 clusters applied on
  % non-consorting pairs.}
  % \label{fig:non_consort_clustering_vis}
% \end{figure}
\begin{figure}[h]
      \centering
          \includegraphics[width=0.5\textwidth]{../figs/consort_kmeans_2d_tsne.png}
  \caption{t-SNE visualization of K-Means with 4 clusters applied on consorting
  pairs.}
  \label{fig:consort_clustering_vis}
\end{figure}

The lower-dimensional visualization of consorting clusters  (Figure
\ref{fig:consort_clustering_vis}) shows cleaner separation for the consorting
pairs than for the non-consorting pairs. Our initial experimentation with
affinity propagation led the number of automatically determined clusters
resulted in over 64 clusters, which is too high for us to interpret reasonably.

\subsection{Graphical Approach}

%------------------------------------------------
Of the three methods using graph theory that we implemented none performed
particularly well. For all methods, we used 5 folds of cross-validation
to get test data and averaged across those folds. We always kept at least one
one edge between each pair of nodes in the training data. For the latent edge
prediction we got a F1 score of .08, for the latent edge prediction only using
edges where one of the endpoints is one of the nodes in the test example we got
a F1 score of .25. Using locally weighted regression, we got an F1 score of
.24. While none of these F1 scores are particularly good the more specific
latent edge prediction is noteworthy in that about one third of examples there
were no positive classes in the utilized training examples. This guarantees
that it would predict a negative class, which would drastically hurt the F1
score. A method to avoid this, such as a back-off model that would
fallback to another model in such cases would improve our results.
\section{Next Steps}

The analysis that we have performed until now used the features directly from
the original dataset. We plan to examine if appyling additional preprocessing to
the data, such as normalizing ages or applying whitening to the derived features
can improve our algorithms.\\

The results obtained through a first pass of linear and non-linear classifiers
do not measure well in terms of the defined metrics, leaving a large margin of
improvement in terms of the chosen algorithms and features. Moreover, further
investigation is necessary to fully understand  the reason why such algorithms
have performed poorly and why they may or may not be adequate for this
particular data. \\

Additionally, we plan to characterize clusters based on examining the cluster
centroids. Also, we plan to continue investigating clustering methods,
especially those that can automatically determine the number of clusters within
our data, without creating an excessive number of clusters. One potential
approach may be to use silhouette scores to assign costs for a number of
clusters.\\

For graphical analysis we will look at models that ignore many of the edge
attributes and instead focus on what sort of information can be gained purely
from the graph structure. Specifically we would like to adapt status and
balance theory to work on bipartite graphs. While these models are unlikely to
be good classifiers in and of themselves, they will utilize features that other
models such as SVMs cannot process. We expect ensemble methods with SVMs and
graph models to provide better performance than using either in isolation.

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[Tung et al., 2012]{Tung:2012}
  Jenny Tung, Marie J. E. Charpentier, Sayan Mukherjee, Jeanne Altmann, and Susan C. Alberts (2012)
\newblock 
  Genetic Effects on Mating Success and Partner Choice in a Social Mammal.
\newblock {\em The American Naturalist}, 2012 180:1, 113--129.

\bibitem [Macskassy, 2007]{Macskassy:2007} 
Sofus A. Macskassy (2007) 
\newblock
Improving learning in networked data by combining explicit and mined links
\newblock{\em Proceedings of the 22nd national conference on Artificial intelligence}, July 22-26, 2007, p.590-595
 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
